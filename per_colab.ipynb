{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/mariprati/notebooks/blob/main/main_.ipynb",
      "authorship_tag": "ABX9TyN/ufmva+xsiDiKlm05EtyO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71af86367e334a8e8ba4b6d66b26c3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_372363d0ce3647b89f1af92c2a1da331",
              "IPY_MODEL_75c276d635d94d69be89a21fdf829ed5",
              "IPY_MODEL_21f3badafd1948e6a0a8b54e9995ff21"
            ],
            "layout": "IPY_MODEL_84a60af766e6420a93d5551cdd8da63e"
          }
        },
        "372363d0ce3647b89f1af92c2a1da331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e67aec051f6a4db685ab4bad26b28c1a",
            "placeholder": "​",
            "style": "IPY_MODEL_06bf26f4b37648f595c6b8c720127381",
            "value": "100%"
          }
        },
        "75c276d635d94d69be89a21fdf829ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f63f22cbf1bb49c489ab0bb6f5199307",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ee6aa6c7cd6454a99142443c70952fb",
            "value": 5
          }
        },
        "21f3badafd1948e6a0a8b54e9995ff21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c516faa25fe40e3852a85fff8a6bee9",
            "placeholder": "​",
            "style": "IPY_MODEL_6452c3082b9949898a0bade1ab2bc262",
            "value": " 5/5 [00:02&lt;00:00,  1.32it/s]"
          }
        },
        "84a60af766e6420a93d5551cdd8da63e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e67aec051f6a4db685ab4bad26b28c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06bf26f4b37648f595c6b8c720127381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f63f22cbf1bb49c489ab0bb6f5199307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ee6aa6c7cd6454a99142443c70952fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c516faa25fe40e3852a85fff8a6bee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6452c3082b9949898a0bade1ab2bc262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariprati/notebooks/blob/main/per_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dicom2nifti\n",
        "!pip install nibabel\n",
        "!pip install totalsegmentator\n",
        "!pip install shutil\n",
        "!pip install pydicom\n",
        "!pip install matplotlib\n",
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jt2YSAHj1fCp",
        "outputId": "6aa06fa8-ba26-4af7-9737-63269f84ec17"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dicom2nifti\n",
            "  Downloading dicom2nifti-2.5.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from dicom2nifti) (5.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dicom2nifti) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dicom2nifti) (1.13.1)\n",
            "Collecting pydicom>=2.2.0 (from dicom2nifti)\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting python-gdcm (from dicom2nifti)\n",
            "  Downloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel->dicom2nifti) (6.4.5)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from nibabel->dicom2nifti) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from nibabel->dicom2nifti) (4.12.2)\n",
            "Downloading dicom2nifti-2.5.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-gdcm, pydicom, dicom2nifti\n",
            "Successfully installed dicom2nifti-2.5.0 pydicom-3.0.1 python-gdcm-3.0.24.1\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (5.3.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel) (6.4.5)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from nibabel) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from nibabel) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from nibabel) (4.12.2)\n",
            "Collecting totalsegmentator\n",
            "  Downloading TotalSegmentator-2.4.0-py3-none-any.whl.metadata (972 bytes)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from totalsegmentator) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from totalsegmentator) (1.26.4)\n",
            "Collecting SimpleITK (from totalsegmentator)\n",
            "  Downloading SimpleITK-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: nibabel>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from totalsegmentator) (5.3.2)\n",
            "Requirement already satisfied: tqdm>=4.45.0 in /usr/local/lib/python3.10/dist-packages (from totalsegmentator) (4.66.6)\n",
            "Collecting p-tqdm (from totalsegmentator)\n",
            "  Downloading p_tqdm-1.4.2.tar.gz (6.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xvfbwrapper (from totalsegmentator)\n",
            "  Downloading xvfbwrapper-0.2.9.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nnunetv2>=2.2.1 (from totalsegmentator)\n",
            "  Downloading nnunetv2-2.5.1.tar.gz (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.0/197.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rt-utils (from totalsegmentator)\n",
            "  Downloading rt_utils-1.2.7-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: dicom2nifti in /usr/local/lib/python3.10/dist-packages (from totalsegmentator) (2.5.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from totalsegmentator) (17.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from totalsegmentator) (2.32.3)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel>=2.3.0->totalsegmentator) (6.4.5)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from nibabel>=2.3.0->totalsegmentator) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from nibabel>=2.3.0->totalsegmentator) (4.12.2)\n",
            "Collecting acvl-utils<0.3,>=0.2 (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading acvl_utils-0.2.2.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dynamic-network-architectures<0.4,>=0.3.1 (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading dynamic_network_architectures-0.3.1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (1.13.1)\n",
            "Collecting batchgenerators>=0.25 (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (1.5.2)\n",
            "Requirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (0.24.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (2.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (0.20.3)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.10/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (2024.9.20)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (0.13.2)\n",
            "Collecting imagecodecs (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting yacs (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Collecting batchgeneratorsv2>=0.2 (from nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading batchgeneratorsv2-0.2.1.tar.gz (34 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nnunetv2>=2.2.1->totalsegmentator) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->totalsegmentator) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->totalsegmentator) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->totalsegmentator) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->totalsegmentator) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->totalsegmentator) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->totalsegmentator) (1.3.0)\n",
            "Requirement already satisfied: pydicom>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from dicom2nifti->totalsegmentator) (3.0.1)\n",
            "Requirement already satisfied: python-gdcm in /usr/local/lib/python3.10/dist-packages (from dicom2nifti->totalsegmentator) (3.0.24.1)\n",
            "Collecting pathos>=0.2.5 (from p-tqdm->totalsegmentator)\n",
            "  Downloading pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from p-tqdm->totalsegmentator) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->totalsegmentator) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->totalsegmentator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->totalsegmentator) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->totalsegmentator) (2024.8.30)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from rt-utils->totalsegmentator) (4.10.0.84)\n",
            "Collecting dataclasses (from rt-utils->totalsegmentator)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting connected-components-3d (from acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading connected_components_3d-3.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Collecting blosc2>=3.0.0b4 (from acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading blosc2-3.0.0b4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator) (11.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator) (1.0.0)\n",
            "Collecting unittest2 (from batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator) (3.5.0)\n",
            "Collecting fft-conv-pytorch (from batchgeneratorsv2>=0.2->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting ppft>=1.7.6.9 (from pathos>=0.2.5->p-tqdm->totalsegmentator)\n",
            "  Downloading ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dill>=0.3.9 (from pathos>=0.2.5->p-tqdm->totalsegmentator)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pox>=0.3.5 (from pathos>=0.2.5->p-tqdm->totalsegmentator)\n",
            "  Downloading pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.17 (from pathos>=0.2.5->p-tqdm->totalsegmentator)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2>=2.2.1->totalsegmentator) (2.36.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2>=2.2.1->totalsegmentator) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->totalsegmentator) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2>=2.2.1->totalsegmentator) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2>=2.2.1->totalsegmentator) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2>=2.2.1->totalsegmentator) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2>=2.2.1->totalsegmentator) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2>=2.2.1->totalsegmentator) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2>=2.2.1->totalsegmentator) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nnunetv2>=2.2.1->totalsegmentator) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->nnunetv2>=2.2.1->totalsegmentator) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nnunetv2>=2.2.1->totalsegmentator) (1.4.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs->nnunetv2>=2.2.1->totalsegmentator) (6.0.2)\n",
            "Requirement already satisfied: ndindex in /usr/local/lib/python3.10/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (1.9.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (1.1.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (2.10.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (9.0.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (0.27.2)\n",
            "Collecting argparse (from unittest2->batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting traceback2 (from unittest2->batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (0.14.0)\n",
            "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25->nnunetv2>=2.2.1->totalsegmentator)\n",
            "  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2>=2.2.1->totalsegmentator) (1.2.2)\n",
            "Downloading TotalSegmentator-2.4.0-py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.5/347.5 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SimpleITK-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rt_utils-1.2.7-py2.py3-none-any.whl (18 kB)\n",
            "Downloading pathos-0.3.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Downloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading blosc2-3.0.0b4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pox-0.3.5-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.6.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading connected_components_3d-3.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: nnunetv2, p-tqdm, xvfbwrapper, acvl-utils, batchgenerators, batchgeneratorsv2, dynamic-network-architectures\n",
            "  Building wheel for nnunetv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nnunetv2: filename=nnunetv2-2.5.1-py3-none-any.whl size=264367 sha256=23a7b81021502b03088c42692e9b29ed4eab1c5909067f9571b6ba35adc608b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/d6/90/88743b341922dc9f6795742570aac83a1eaa55f77ee676a5a6\n",
            "  Building wheel for p-tqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for p-tqdm: filename=p_tqdm-1.4.2-py3-none-any.whl size=5400 sha256=eeaaa351df32c9ca572be14b53568c257fe604857ea494ca64d5da09122917d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/eb/46/364fe18b96fa8438176535990d8c8ac9ac87becde4cd340d3d\n",
            "  Building wheel for xvfbwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xvfbwrapper: filename=xvfbwrapper-0.2.9-py3-none-any.whl size=5010 sha256=846402fc6ed47bd87ab301db5accd56e9b69303efe38d0bea3ead04bd30fb442\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/10/7d/2b7fdffccf837f7d5425931575fbee9caebe2c190931f9058b\n",
            "  Building wheel for acvl-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for acvl-utils: filename=acvl_utils-0.2.2-py3-none-any.whl size=24723 sha256=7ca0c25eb3eb10b782163c61efc5bf8066b062c189564bbf45a20776042a7978\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/56/f0/c3ece6950db0c5fb6ad37b5ce1406a4a1b035840c468c53202\n",
            "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93088 sha256=4144f3d898d2bcefbc2811f6e27eba590932b35b755211151618f4867e185d4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/1b/30/b3f066999ad01855fc903fe7c93c25682333dd5645d5c75434\n",
            "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.2.1-py3-none-any.whl size=45185 sha256=a5a48ce28c0ccf44b940156d4bf5713363d93d934627b0b82ee72b2263d3ca82\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/91/33993997db216e7b946d379850c47837d2478be49377a6cb41\n",
            "  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.3.1-py3-none-any.whl size=30046 sha256=d61e5d79b027061d403f17c8bb007f06db319ea7752c915b4a49ef349648b689\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/1b/13/a6419c8dbf998b9343710355ec3edc5c8e24d9b7b22eec95fb\n",
            "Successfully built nnunetv2 p-tqdm xvfbwrapper acvl-utils batchgenerators batchgeneratorsv2 dynamic-network-architectures\n",
            "Installing collected packages: xvfbwrapper, SimpleITK, linecache2, dataclasses, argparse, yacs, traceback2, ppft, pox, imagecodecs, dill, connected-components-3d, unittest2, rt-utils, multiprocess, pathos, fft-conv-pytorch, dynamic-network-architectures, blosc2, batchgenerators, p-tqdm, batchgeneratorsv2, acvl-utils, nnunetv2, totalsegmentator\n",
            "  Attempting uninstall: blosc2\n",
            "    Found existing installation: blosc2 2.7.1\n",
            "    Uninstalling blosc2-2.7.1:\n",
            "      Successfully uninstalled blosc2-2.7.1\n",
            "Successfully installed SimpleITK-2.4.0 acvl-utils-0.2.2 argparse-1.4.0 batchgenerators-0.25.1 batchgeneratorsv2-0.2.1 blosc2-3.0.0b4 connected-components-3d-3.20.0 dataclasses-0.6 dill-0.3.9 dynamic-network-architectures-0.3.1 fft-conv-pytorch-1.2.0 imagecodecs-2024.9.22 linecache2-1.0.0 multiprocess-0.70.17 nnunetv2-2.5.1 p-tqdm-1.4.2 pathos-0.3.3 pox-0.3.5 ppft-1.7.6.9 rt-utils-1.2.7 totalsegmentator-2.4.0 traceback2-1.4.0 unittest2-1.1.0 xvfbwrapper-0.2.9 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "dataclasses"
                ]
              },
              "id": "516b0993a1c0485bb05f4915e4fde92b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for shutil\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pydicom in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TLhSxbnO0lF1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dicom2nifti\n",
        "import nibabel as nib\n",
        "from totalsegmentator.python_api import totalsegmentator\n",
        "import shutil\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import dicom2nifti\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% FUNCTIONS\n",
        "\n",
        "def dicom_folder_to_nifti(dicom_folder, output_folder):\n",
        "    \"\"\"Converts a directory of DICOM files to a NIfTI volume and saves it using dicom2nifti.\"\"\"\n",
        "    folder_name = os.path.basename(os.path.normpath(dicom_folder))\n",
        "    filename = f\"{folder_name}_nifti_volume.nii\"\n",
        "\n",
        "    # Convert the DICOM series in the specified directory to NIfTI\n",
        "    dicom2nifti.dicom_series_to_nifti(dicom_folder, os.path.join(output_folder, filename), reorient_nifti=True)\n",
        "\n",
        "    print(f\"Saved NIfTI volume at {os.path.join(output_folder, filename)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BFSm4jHA1BrF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nifti_info(nifti_path):\n",
        "    \"\"\"Extracts and prints the dimensions and metadata of a NIfTI file.\"\"\"\n",
        "    # Load the NIfTI image\n",
        "    nifti_img = nib.load(nifti_path)\n",
        "\n",
        "    # Get the data array dimensions\n",
        "    dimensions = nifti_img.shape  # This gives the number of pixels along each axis\n",
        "\n",
        "    # Get the voxel size\n",
        "    voxel_sizes = nifti_img.header.get_zooms()  # Pixel size along each axis in millimeters\n",
        "\n",
        "    # Print the information\n",
        "    print(f\"Dimensions (pixels along each axis): {dimensions}\")\n",
        "    print(f\"Voxel sizes (mm per axis): {voxel_sizes}\")\n",
        "    print(f\"Data type: {nifti_img.get_data_dtype()}\")\n",
        "    print(f\"Affine transformation matrix:\\n{nifti_img.affine}\")\n",
        "\n",
        "    return {\n",
        "        \"dimensions\": dimensions,\n",
        "        \"voxel_sizes\": voxel_sizes,\n",
        "        \"data_type\": nifti_img.get_data_dtype(),\n",
        "        \"affine\": nifti_img.affine\n",
        "    }"
      ],
      "metadata": {
        "id": "vjaV1KRW1EM4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_and_save_slices(dicom_folder, target_value):\n",
        "    \"\"\"\n",
        "    Finds the DICOM slice with a specific InstanceNumber, extracts its\n",
        "    ImagePatientPosition, and returns it as a vector (x, y, z).\n",
        "\n",
        "    Parameters:\n",
        "        dicom_folder (str): Path to the folder containing DICOM files.\n",
        "        target_value (int): The target InstanceNumber to find.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The ImagePatientPosition as a tuple (x, y, z).\n",
        "    \"\"\"\n",
        "    # Load all DICOM files in the folder and sort by Instance Number\n",
        "    dicom_files = sorted(\n",
        "        [f for f in os.listdir(dicom_folder) if f.endswith(\".dcm\")],\n",
        "        key=lambda x: pydicom.dcmread(os.path.join(dicom_folder, x)).InstanceNumber\n",
        "    )\n",
        "\n",
        "    target_index = None\n",
        "    img_patient_position = None\n",
        "\n",
        "    # Iterate over DICOM files to find the slice with InstanceNumber equal to target_value\n",
        "    for i, filename in enumerate(dicom_files):\n",
        "        filepath = os.path.join(dicom_folder, filename)\n",
        "        dicom_data = pydicom.dcmread(filepath)\n",
        "\n",
        "        # Check if the InstanceNumber matches the target value\n",
        "        if dicom_data.InstanceNumber == target_value:\n",
        "            # Extract the ImagePatientPosition of this slice\n",
        "            img_patient_position = dicom_data.ImagePositionPatient  # DICOM tag (0020,0032)\n",
        "            target_index = i\n",
        "            break\n",
        "\n",
        "    # If target slice is not found, exit\n",
        "    if target_index is None:\n",
        "        print(f\"No slice found with InstanceNumber equal to {target_value}\")\n",
        "        return None\n",
        "\n",
        "    # If the slice is found, return the ImagePatientPosition as a tuple\n",
        "    x, y, z = img_patient_position\n",
        "    print(f\"Slice with InstanceNumber {target_value} found at index {target_index}.\")\n",
        "    print(f\"ImagePatientPosition: x={x}, y={y}, z={z}\")\n",
        "    return x, y, z\n"
      ],
      "metadata": {
        "id": "QkGI4LRX1Gww"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def world_to_voxel_coordinates(header, x, y, z):\n",
        "    \"\"\"\n",
        "    Convert world coordinates (x, y, z) to voxel indices (i, j, k) using the NIfTI header.\n",
        "\n",
        "    Parameters:\n",
        "    header: NIfTI header object (e.g., from nibabel)\n",
        "    x, y, z: World coordinates (float)\n",
        "\n",
        "    Returns:\n",
        "    i, j, k: Voxel indices (int)\n",
        "    \"\"\"\n",
        "    # Extract necessary fields from the header\n",
        "    pixdim = header[\"pixdim\"]\n",
        "    qoffset_x = header[\"qoffset_x\"]\n",
        "    qoffset_y = header[\"qoffset_y\"]\n",
        "    qoffset_z = header[\"qoffset_z\"]\n",
        "    qfac = pixdim[0]  # Either 1 or -1\n",
        "\n",
        "    # Define the fixed rotation matrix R\n",
        "    R = np.array([\n",
        "        [1, 0,  0],\n",
        "        [0, -1, 0],\n",
        "        [0, 0, -1]\n",
        "    ])\n",
        "\n",
        "    # Subtract offsets to account for translation\n",
        "    world_vector = np.array([x, y, z]) - np.array([qoffset_x, qoffset_y, qoffset_z])\n",
        "\n",
        "    # Invert the rotation matrix (which is simple since it's a diagonal matrix)\n",
        "    R_inv = np.linalg.inv(R)\n",
        "\n",
        "    # Apply the inverse rotation\n",
        "    voxel_vector = R_inv @ world_vector\n",
        "\n",
        "    # Divide by the scaling factors to get the voxel indices\n",
        "    i = voxel_vector[0] / pixdim[1]\n",
        "    j = voxel_vector[1] / pixdim[2]\n",
        "    k = voxel_vector[2] / (qfac * pixdim[3])\n",
        "\n",
        "    # Round to the nearest integer to get the voxel indices\n",
        "    i = round(i)\n",
        "    j = round(j)\n",
        "    k = round(k)\n",
        "\n",
        "    return i, j, k"
      ],
      "metadata": {
        "id": "lXFd9rvX1Jln"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sct_slice_number(csv_file, pid, study_yr):\n",
        "    \"\"\"\n",
        "    Finds the sct_slice_number for the given pid and study_yr from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): Path to the CSV file.\n",
        "        pid (str): Patient ID.\n",
        "        study_yr (str): Study year.\n",
        "\n",
        "    Returns:\n",
        "        int: The corresponding sct_slice_number.\n",
        "    \"\"\"\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Filter the DataFrame for the specific pid and study_yr\n",
        "    row = df[(df['pid'] == pid) & (df['study_yr'] == study_yr)]\n",
        "\n",
        "    # Check if the row exists and retrieve sct_slice_number\n",
        "    if not row.empty:\n",
        "        print(f\"Matching row from CSV:\\n{row}\\n\")\n",
        "        return int(row['sct_slice_num'].values[0])\n",
        "    else:\n",
        "        raise ValueError(f\"No matching entry found for pid={pid} and study_yr={study_yr}\")\n"
      ],
      "metadata": {
        "id": "PHqRNZj_1MbR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sct_epi_loc(csv_file, pid, study_yr):\n",
        "    \"\"\"\n",
        "    Finds the sct_epi_loc for the given pid and study_yr from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): Path to the CSV file.\n",
        "        pid (str): Patient ID.\n",
        "        study_yr (str): Study year.\n",
        "\n",
        "    Returns:\n",
        "        int: The corresponding sct_epi_loc.\n",
        "    \"\"\"\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Filter the DataFrame for the specific pid and study_yr\n",
        "    row = df[(df['pid'] == pid) & (df['study_yr'] == study_yr)]\n",
        "\n",
        "    # Check if the row exists and retrieve sct_epi_loc\n",
        "    if not row.empty:\n",
        "        sct_epi_loc = int(row['sct_epi_loc'].values[0])\n",
        "        print(f\"Matching row from CSV:\\n{row}\\n\")\n",
        "        print(f\"sct_epi_loc value: {sct_epi_loc}\")\n",
        "        return sct_epi_loc\n",
        "    else:\n",
        "        raise ValueError(f\"No matching entry found for pid={pid} and study_yr={study_yr}\")"
      ],
      "metadata": {
        "id": "imF17BNu1PjY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_lung_lobe_nifti(masked_output_dir, folder_name, sct_epi_loc):\n",
        "    \"\"\"\n",
        "    Loads the NIfTI file corresponding to the lobe based on sct_epi_loc and returns the region name.\n",
        "\n",
        "    Args:\n",
        "        masked_output_dir (str): Directory containing the masked NIfTI files.\n",
        "        folder_name (str): Name of the folder containing the DICOM data.\n",
        "        sct_epi_loc (int): Value determining which lobe to load.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - nib.Nifti1Image: The loaded NIfTI file.\n",
        "            - str: The name of the region (e.g., 'lung_upper_lobe_right').\n",
        "    \"\"\"\n",
        "    nifti_file = None\n",
        "    region = None\n",
        "\n",
        "    # Load the appropriate file based on sct_epi_loc\n",
        "    if sct_epi_loc == 1:\n",
        "        region = 'lung_upper_lobe_right'\n",
        "        nifti_file = nib.load(os.path.join(masked_output_dir, f'{region}_{folder_name}.nii'))\n",
        "        print(\"Loaded Right Upper Lobe\")\n",
        "    elif sct_epi_loc == 2:\n",
        "        region = 'lung_middle_lobe_right'\n",
        "        nifti_file = nib.load(os.path.join(masked_output_dir, f'{region}_{folder_name}.nii'))\n",
        "        print(\"Loaded Right Middle Lobe\")\n",
        "    elif sct_epi_loc == 3:\n",
        "        region = 'lung_lower_lobe_right'\n",
        "        nifti_file = nib.load(os.path.join(masked_output_dir, f'{region}_{folder_name}.nii'))\n",
        "        print(\"Loaded Right Lower Lobe\")\n",
        "    elif sct_epi_loc == 4:\n",
        "        region = 'lung_upper_lobe_left'\n",
        "        nifti_file = nib.load(os.path.join(masked_output_dir, f'{region}_{folder_name}.nii'))\n",
        "        print(\"Loaded Left Upper Lobe\")\n",
        "    elif sct_epi_loc in [5, 6]:  # Lingula (5) treated as part of Left Lower Lobe (6)\n",
        "        region = 'lung_lower_lobe_left'\n",
        "        nifti_file = nib.load(os.path.join(masked_output_dir, f'{region}_{folder_name}.nii'))\n",
        "        print(\"Loaded Left Lower Lobe (including Lingula)\")\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid sct_epi_loc value: {sct_epi_loc}\")\n",
        "\n",
        "    return nifti_file, region"
      ],
      "metadata": {
        "id": "2qnTBym61Rex"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% MAIN\n",
        "\n",
        "\n",
        "def main():\n",
        "    # %% Set environment variables at the start of the script\n",
        "    os.environ['LC_ALL'] = 'C.UTF-8'\n",
        "    os.environ['LANG'] = 'C.UTF-8'\n",
        "    os.environ['LANGUAGE'] = 'C.UTF-8'\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------------------------\n",
        "    # %% SEGMENTAZIONE\n",
        "    # --------------------------------------------------------------------------------------------\n",
        "    dicom_folder = '/content/104402_t1' # UNICA DA LASCIARE\n",
        "    output_folder = '/content/res'\n",
        "\n",
        "    # Extract the last part of the dicom_folder path (the folder name)\n",
        "    folder_name = os.path.basename(os.path.normpath(dicom_folder))\n",
        "\n",
        "    # Create a new output folder path inside Segmentazione2\n",
        "    output_folder = os.path.join(output_folder, folder_name)\n",
        "\n",
        "    # Ensure the new output folder exists\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    dicom_folder_to_nifti(dicom_folder, output_folder)\n",
        "\n",
        "    # Define paths for the segmentation\n",
        "    input_image_path = os.path.join(output_folder, f\"{folder_name}_nifti_volume.nii\")\n",
        "    output_dir = \"/content/104402_t1/res/Lung_segmentations\"\n",
        "\n",
        "    # Now, create the 'segmentazione_vcs' folder inside the newly updated output folder\n",
        "    output_dir = os.path.join(output_folder, 'Lung_segmentations')\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Define the lung lobe names\n",
        "    lobe_names = [\n",
        "        'lung_upper_lobe_left',\n",
        "        'lung_lower_lobe_left',\n",
        "        'lung_upper_lobe_right',\n",
        "        'lung_middle_lobe_right',\n",
        "        'lung_lower_lobe_right'\n",
        "    ]\n",
        "    labels = {\n",
        "        'lung_upper_lobe_left': 1,\n",
        "        'lung_lower_lobe_left': 2,\n",
        "        'lung_upper_lobe_right': 3,\n",
        "        'lung_middle_lobe_right': 4,\n",
        "        'lung_lower_lobe_right': 5\n",
        "    }\n",
        "\n",
        "    # Segment the lung lobes using TotalSegmentator\n",
        "    totalsegmentator(\n",
        "        input=input_image_path,\n",
        "        output=output_dir,\n",
        "        # fast=True,\n",
        "        device=\"cpu\",\n",
        "        task=\"total\",\n",
        "        roi_subset=lobe_names\n",
        "    )\n",
        "\n",
        "    # Load the original CT image\n",
        "    ct_img = nib.load(input_image_path)\n",
        "    ct_data = ct_img.get_fdata()\n",
        "\n",
        "    # Output directory for masked images\n",
        "    masked_output_dir = os.path.join(output_dir, 'Lung_segmented_masks')\n",
        "    os.makedirs(masked_output_dir, exist_ok=True)\n",
        "\n",
        "    # Create and save masked images for each region\n",
        "    for region, label in labels.items():\n",
        "        seg_path = os.path.join(output_dir, f'{region}.nii.gz')\n",
        "        if os.path.exists(seg_path):\n",
        "            # Load the specific segmentation file for the current region\n",
        "            seg_img = nib.load(seg_path)\n",
        "            seg_data = seg_img.get_fdata()\n",
        "\n",
        "            # Create a mask for the current region\n",
        "            mask = seg_data == 1  # Assuming the segmentation file has binary mask (1 for region, 0 for background)\n",
        "\n",
        "            # Apply the mask to the original CT data, setting background to -1000\n",
        "            masked_ct_data = np.where(mask, ct_data, -1000)\n",
        "\n",
        "            # Create a new NIfTI image for the masked data\n",
        "            masked_ct_img = nib.Nifti1Image(masked_ct_data, ct_img.affine, ct_img.header)\n",
        "\n",
        "            # Save the masked image\n",
        "            output_path = os.path.join(masked_output_dir, f'{region}_{folder_name}.nii')\n",
        "            nib.save(masked_ct_img, output_path)\n",
        "        else:\n",
        "            print(f\"Segmentation file for {region}_{folder_name} not found at {seg_path}\")\n",
        "    # Print the dimensions of the segmented NIfTI files\n",
        "    for region in labels.keys():\n",
        "        seg_path = os.path.join(masked_output_dir, f'{region}_{folder_name}.nii')\n",
        "        if os.path.exists(seg_path):\n",
        "            print(f\"Dimensions for {region}_{folder_name}:\")\n",
        "            get_nifti_info(seg_path)\n",
        "        else:\n",
        "            print(f\"Segmentation file for {region}_{folder_name} not found at {seg_path}\")\n",
        "    print(\"Masked NIfTI files have been saved.\")\n",
        "\n",
        "    # --------------------------------------------------------------------------------------------\n",
        "    # %% SUBVOLUME\n",
        "    # --------------------------------------------------------------------------------------------\n",
        "\n",
        "    csv_file = \"/content/unique_filter_51_total.csv\"  # Path to your CSV file\n",
        "    pid = 104402\n",
        "    study_yr = 1\n",
        "    sct_epi_loc = find_sct_epi_loc(csv_file, pid, study_yr)\n",
        "    nifti_file, region = load_lung_lobe_nifti(masked_output_dir, folder_name, sct_epi_loc)\n",
        "\n",
        "    print(f\"Loaded NIfTI file: {nifti_file}\")\n",
        "    print(f\"Corresponding region: {region}\")\n",
        "\n",
        "    header = nifti_file.header\n",
        "    affine = nifti_file.affine\n",
        "    data = nifti_file.get_fdata()\n",
        "    pixdim = header[\"pixdim\"]\n",
        "    dimen = header[\"dim\"]\n",
        "    sct_slice_number = find_sct_slice_number(csv_file, pid, study_yr)\n",
        "    result = find_and_save_slices(dicom_folder, sct_slice_number)\n",
        "\n",
        "    if result:\n",
        "        x, y, z = result\n",
        "        print(f\"Target slice ImagePatientPosition: x={x}, y={y}, z={z}\")\n",
        "    else:\n",
        "        print(\"No slice found.\")\n",
        "    print(x,y,z)\n",
        "\n",
        "    i, j, k = world_to_voxel_coordinates(header, x, y, z)\n",
        "\n",
        "    # Extract the slice from the data (for example, slice at 'k' in the z-direction)\n",
        "    slice_at_z = data[:, :, k]\n",
        "\n",
        "    # Plot the slice\n",
        "    # plt.imshow(slice_at_z.T, cmap=\"gray\")  # Transpose for correct orientation\n",
        "    # plt.title(f\"Slice at Z = {z}\")\n",
        "    # plt.show()\n",
        "\n",
        "    # Define the window size around the slice\n",
        "    window_size = 1  # Window of ±1 slices\n",
        "    k_min = k - window_size\n",
        "    k_max = k + window_size + 1\n",
        "\n",
        "    # Extract the sub-volume\n",
        "    sub_volume = data[:, :, k_min:k_max]\n",
        "\n",
        "    # Adjust the affine matrix for the sub-volume\n",
        "    new_affine = affine.copy()\n",
        "    new_affine[:3, 3] += k_min * pixdim[3]  # Adjust the Z offset in the affine matrix\n",
        "\n",
        "    # Create a new NIfTI object for the sub-volume\n",
        "    new_nifti = nib.Nifti1Image(sub_volume, affine=new_affine, header=header)\n",
        "\n",
        "    # Full path to the output file\n",
        "    output_path = os.path.join(masked_output_dir, f\"lung_subvolume_{folder_name}_{region}.nii\")\n",
        "\n",
        "\n",
        "    # Save the new NIfTI file\n",
        "    #output_path = \"/Users/mariaprati/Desktop/TESI/Segmentazione2/100176_t1/Lung_segmentations/Lung_segmented_masks/lung_subvolume.nii\" #QUI POI METTERE NOME\n",
        "    nib.save(new_nifti, output_path)\n",
        "    print(f\"Sottovolume salvato in: {output_path}\")\n",
        "\n",
        "    # Load the saved sub-volume NIfTI file\n",
        "    subvolume_nifti = nib.load(output_path)\n",
        "    subvolume_data = subvolume_nifti.get_fdata()\n",
        "\n",
        "    # Print the dimensions of the sub-volume\n",
        "    print(\"Dimensions of the lung sub-volume:\", subvolume_data.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wDkVSUBi1UrL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "71af86367e334a8e8ba4b6d66b26c3d7",
            "372363d0ce3647b89f1af92c2a1da331",
            "75c276d635d94d69be89a21fdf829ed5",
            "21f3badafd1948e6a0a8b54e9995ff21",
            "84a60af766e6420a93d5551cdd8da63e",
            "e67aec051f6a4db685ab4bad26b28c1a",
            "06bf26f4b37648f595c6b8c720127381",
            "f63f22cbf1bb49c489ab0bb6f5199307",
            "5ee6aa6c7cd6454a99142443c70952fb",
            "1c516faa25fe40e3852a85fff8a6bee9",
            "6452c3082b9949898a0bade1ab2bc262"
          ]
        },
        "id": "BLsvK_Od1ZQA",
        "outputId": "2db0d196-ad72-438d-a274-0457457e357e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved NIfTI volume at /content/res/104402_t1/104402_t1_nifti_volume.nii\n",
            "\n",
            "If you use this tool please cite: https://pubs.rsna.org/doi/10.1148/ryai.230024\n",
            "\n",
            "TotalSegmentator sends anonymous usage statistics. If you want to disable it check the documentation.\n",
            "Downloading model for Task 291 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 234M/234M [00:07<00:00, 32.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download finished. Extracting...\n",
            "Downloading model for Task 292 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 234M/234M [00:08<00:00, 26.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download finished. Extracting...\n",
            "Downloading model for Task 293 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 234M/234M [00:06<00:00, 37.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download finished. Extracting...\n",
            "Downloading model for Task 294 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 234M/234M [00:06<00:00, 35.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download finished. Extracting...\n",
            "Downloading model for Task 295 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 234M/234M [00:07<00:00, 30.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download finished. Extracting...\n",
            "Generating rough segmentation for cropping...\n",
            "Downloading model for Task 298 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 135M/135M [00:08<00:00, 16.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download finished. Extracting...\n",
            "Resampling...\n",
            "  Resampled in 0.61s\n",
            "Predicting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nnunetv2/inference/predict_from_raw_data.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(join(model_training_output_dir, f'fold_{f}', checkpoint_name),\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Predicted in 16.92s\n",
            "Resampling...\n",
            "  cropping from (512, 512, 58) to (512, 413, 56)\n",
            "Resampling...\n",
            "  Resampled in 2.67s\n",
            "Predicting part 1 of 1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nnunetv2/inference/predict_from_raw_data.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(join(model_training_output_dir, f'fold_{f}', checkpoint_name),\n",
            "100%|██████████| 8/8 [02:43<00:00, 20.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Predicted in 179.41s\n",
            "Resampling...\n",
            "Saving segmentations...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71af86367e334a8e8ba4b6d66b26c3d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Saved in 4.31s\n",
            "Dimensions for lung_upper_lobe_left_104402_t1:\n",
            "Dimensions (pixels along each axis): (512, 512, 58)\n",
            "Voxel sizes (mm per axis): (0.5253906, 0.5253906, 5.0)\n",
            "Data type: int16\n",
            "Affine transformation matrix:\n",
            "[[  -0.52539062    0.            0.          134.23730469]\n",
            " [  -0.            0.52539062    0.           33.76268768]\n",
            " [   0.           -0.            5.         -453.        ]\n",
            " [   0.            0.            0.            1.        ]]\n",
            "Dimensions for lung_lower_lobe_left_104402_t1:\n",
            "Dimensions (pixels along each axis): (512, 512, 58)\n",
            "Voxel sizes (mm per axis): (0.5253906, 0.5253906, 5.0)\n",
            "Data type: int16\n",
            "Affine transformation matrix:\n",
            "[[  -0.52539062    0.            0.          134.23730469]\n",
            " [  -0.            0.52539062    0.           33.76268768]\n",
            " [   0.           -0.            5.         -453.        ]\n",
            " [   0.            0.            0.            1.        ]]\n",
            "Dimensions for lung_upper_lobe_right_104402_t1:\n",
            "Dimensions (pixels along each axis): (512, 512, 58)\n",
            "Voxel sizes (mm per axis): (0.5253906, 0.5253906, 5.0)\n",
            "Data type: int16\n",
            "Affine transformation matrix:\n",
            "[[  -0.52539062    0.            0.          134.23730469]\n",
            " [  -0.            0.52539062    0.           33.76268768]\n",
            " [   0.           -0.            5.         -453.        ]\n",
            " [   0.            0.            0.            1.        ]]\n",
            "Dimensions for lung_middle_lobe_right_104402_t1:\n",
            "Dimensions (pixels along each axis): (512, 512, 58)\n",
            "Voxel sizes (mm per axis): (0.5253906, 0.5253906, 5.0)\n",
            "Data type: int16\n",
            "Affine transformation matrix:\n",
            "[[  -0.52539062    0.            0.          134.23730469]\n",
            " [  -0.            0.52539062    0.           33.76268768]\n",
            " [   0.           -0.            5.         -453.        ]\n",
            " [   0.            0.            0.            1.        ]]\n",
            "Dimensions for lung_lower_lobe_right_104402_t1:\n",
            "Dimensions (pixels along each axis): (512, 512, 58)\n",
            "Voxel sizes (mm per axis): (0.5253906, 0.5253906, 5.0)\n",
            "Data type: int16\n",
            "Affine transformation matrix:\n",
            "[[  -0.52539062    0.            0.          134.23730469]\n",
            " [  -0.            0.52539062    0.           33.76268768]\n",
            " [   0.           -0.            5.         -453.        ]\n",
            " [   0.            0.            0.            1.        ]]\n",
            "Masked NIfTI files have been saved.\n",
            "Matching row from CSV:\n",
            "        dataset_version_x     pid  sct_ab_desc  sct_ab_num  sct_ab_source  \\\n",
            "1751  2011.02.03/04.07.22  104402           51           1              1   \n",
            "\n",
            "      sct_epi_loc  sct_found_after_comp  sct_long_dia  sct_margins  \\\n",
            "1751          6.0                   0.0           4.0          2.0   \n",
            "\n",
            "      sct_perp_dia  ...  effmas  exposure_time  exposure_time_raw    kvp  \\\n",
            "1751           4.0  ...    30.0          500.0              500.0  120.0   \n",
            "\n",
            "     kvp_raw   mas  pitch  pitch_raw  xray_tube_current  xray_tube_current_raw  \n",
            "1751   120.0  30.0    NaN        NaN               60.0                   60.0  \n",
            "\n",
            "[1 rows x 46 columns]\n",
            "\n",
            "sct_epi_loc value: 6\n",
            "Loaded Left Lower Lobe (including Lingula)\n",
            "Loaded NIfTI file: \n",
            "<class 'nibabel.nifti1.Nifti1Image'>\n",
            "data shape (512, 512, 58)\n",
            "affine:\n",
            "[[  -0.52539062    0.            0.          134.23730469]\n",
            " [  -0.            0.52539062    0.           33.76268768]\n",
            " [   0.           -0.            5.         -453.        ]\n",
            " [   0.            0.            0.            1.        ]]\n",
            "metadata:\n",
            "<class 'nibabel.nifti1.Nifti1Header'> object, endian='<'\n",
            "sizeof_hdr      : 348\n",
            "data_type       : b''\n",
            "db_name         : b''\n",
            "extents         : 0\n",
            "session_error   : 0\n",
            "regular         : b''\n",
            "dim_info        : 0\n",
            "dim             : [  3 512 512  58   1   1   1   1]\n",
            "intent_p1       : 0.0\n",
            "intent_p2       : 0.0\n",
            "intent_p3       : 0.0\n",
            "intent_code     : none\n",
            "datatype        : int16\n",
            "bitpix          : 16\n",
            "slice_start     : 0\n",
            "pixdim          : [-1.         0.5253906  0.5253906  5.         1.         1.\n",
            "  1.         1.       ]\n",
            "vox_offset      : 0.0\n",
            "scl_slope       : nan\n",
            "scl_inter       : nan\n",
            "slice_end       : 0\n",
            "slice_code      : unknown\n",
            "xyzt_units      : 2\n",
            "cal_max         : 0.0\n",
            "cal_min         : 0.0\n",
            "slice_duration  : 0.0\n",
            "toffset         : 0.0\n",
            "glmax           : 0\n",
            "glmin           : 0\n",
            "descrip         : b''\n",
            "aux_file        : b''\n",
            "qform_code      : unknown\n",
            "sform_code      : aligned\n",
            "quatern_b       : 0.0\n",
            "quatern_c       : 1.0\n",
            "quatern_d       : 0.0\n",
            "qoffset_x       : 134.2373\n",
            "qoffset_y       : 33.762688\n",
            "qoffset_z       : -453.0\n",
            "srow_x          : [ -0.5253906   0.          0.        134.2373   ]\n",
            "srow_y          : [-0.         0.5253906  0.        33.762688 ]\n",
            "srow_z          : [   0.   -0.    5. -453.]\n",
            "intent_name     : b''\n",
            "magic           : b'n+1'\n",
            "\n",
            "Corresponding region: lung_lower_lobe_left\n",
            "Matching row from CSV:\n",
            "        dataset_version_x     pid  sct_ab_desc  sct_ab_num  sct_ab_source  \\\n",
            "1751  2011.02.03/04.07.22  104402           51           1              1   \n",
            "\n",
            "      sct_epi_loc  sct_found_after_comp  sct_long_dia  sct_margins  \\\n",
            "1751          6.0                   0.0           4.0          2.0   \n",
            "\n",
            "      sct_perp_dia  ...  effmas  exposure_time  exposure_time_raw    kvp  \\\n",
            "1751           4.0  ...    30.0          500.0              500.0  120.0   \n",
            "\n",
            "     kvp_raw   mas  pitch  pitch_raw  xray_tube_current  xray_tube_current_raw  \n",
            "1751   120.0  30.0    NaN        NaN               60.0                   60.0  \n",
            "\n",
            "[1 rows x 46 columns]\n",
            "\n",
            "Slice with InstanceNumber 41 found at index 40.\n",
            "ImagePatientPosition: x=-134.2373, y=-302.2373, z=-368\n",
            "Target slice ImagePatientPosition: x=-134.2373, y=-302.2373, z=-368\n",
            "-134.2373 -302.2373 -368\n",
            "Sottovolume salvato in: /content/res/104402_t1/Lung_segmentations/Lung_segmented_masks/lung_subvolume_104402_t1_lung_lower_lobe_left.nii\n",
            "Dimensions of the lung sub-volume: (512, 512, 3)\n"
          ]
        }
      ]
    }
  ]
}